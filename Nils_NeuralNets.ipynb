{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eine kurze Einführung in neuronale Netze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Text soll eine konzeptionelle Einführung in neuronale Netze bieten. Nach der generellen Erklärung des Konzeptes wird ein einfaches Netz anhand eines Python-Codes veranschaulicht.\n",
    "Als Arbeitsbeispiel dient hier die Erkennung handgeschriebener Ziffern. Diese Aufgabe ist das Standardanwendungsbeispiel für die Einführung in machine learning. Sie ist einerseits sehr intuitiv, andererseits sehr komplex. Für uns ist Ziffernerkennung keine schwere Aufgabe. Unser Gehirn bewältigt sie mehrmals am Tag so problemlos, dass wir meistens nicht mal etwas davon merken. Trotzdem können wir nicht sagen, wie *genau* wir diese Erkennung schaffen; und es wäre vermutlich für die meisten Menschen unmöglich, diesen Prozess in einem Algorithmus zu konkretisieren. Sogar ein \"basales\" neuronales Netz kann diese Aufgabe gut bewältigen. Ein weiterer Vorteil dieses Beispieles ist, dass bereits ein einfach benutzbarer Datensatz für Trainings- und Testmengen existiert, die MNIST database of handwritten digits. Sie beinhaltet 70,000 Bilder handgeschriebener Ziffern. Die einzelnen Bilder sind 28x28 Pixel groß und monochrom. Wir werden sehen, dass keine aufwendige Umstrukturierung der Daten notwendig ist, um sie als Input für unser neuronales Netz zu benutzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die allgemeine Idee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Idee, die hinter neuronalen Netzen steht, ist, einen Algorithmus zu prouzieren, der aus einer großen Datenmenge, den Trainingsbeispielen, lernt. Wir hoffen, dass unser neuronales Netz, wenn wir ihm - in unserem Fall - 60,000 verschiedene Bilder von handgeschriebenen Ziffern zeigen, Muster erkennt und weitere Bilder handgeschriebener Ziffern klassifizieren kann.  \n",
    "Der kleinste Bestandteil eines neuronalen Netzes ist, wie der Name nahelegt, ein Neuron. Erst einmal reicht es, sich ein Neuron als etwas vorzustellen, dass eine Zahl speichert - konkret eine Zahl zwischen 0 und 1. Diese Zahl nennen wir die Aktivierung des Neurons.  \n",
    "Die einzelnen Neuronen des Netzwerkes sind in sogenannten Layern sortiert, sozusagen Vektoren von Neuronen. Das erste Layer ist das Input-Layer. In unserem Fall wird es 784 Neuronen enthalten, die die einzelnen Grauwerte der Pixel des zu klassifizierenden Bildes repräsentieren. Das letzte Layer, das Output-Layer, soll 10 Neuronen enthalten, von denen jeder repräsentiert, wie sicher sich das Netzwerk ist, dass es sich bei dem Bild im Input-Layer um eine bestimmte Zahl handelt. Unsere Hoffnung ist, dass, wenn wir das Bild einer 3 in das Input-Layer speisen (und das Netzwerk genug gelernt hat), die Aktivierung des dritten Neurons im Output-Layer hoch ist und die Aktivierung aller anderen Neuronen niedrig.\n",
    "Zwischen dem Input- und dem Output-Layer liegen noch sogenannte Hidden Layer. Ihre Größe und Anzahl können wir selbst wählen und wir werden versuchen, ein wenig mit ihnen herumzuprobieren und beobachten, wie sich Änderungen dort auf die Effizienz des Netzwerkes auswirken.  \n",
    "Die Aktivierungen eines Layers sollen die Aktivierungen des nächsten Layers beeinflussen. Dafür wird jedes Layer mit dem darauf folgenden durch Kanten verbunden, jedes Neuron im ersten hat eine Kante zu jedem Neuron im zweiten Layer, dort wiederum hat jedes Neuron eine Kante zu jedem Neuron im dritten Layer und so weiter bis zum Output-Layer. Die Kanten sind nicht mehr als Zahlen, Gewichtungen oder weights genannt, die determinieren, wie die Aktivierungen in einem Layer aussehen, und zwar in Abhängigkeit der Aktivierungen des vorherigen Layers. Wir können uns die Kanten zwischen zwei Layern als eine Übergangsmatrix vom vorherigen zum nächsten Layer. Tatsächlich werden die Aktivierungen des nächsten Layers im Grunde so berechnet.  \n",
    "Außerdem bekommt jedes Neuron in den Layern hinter dem Input-Layer einen Bias, eine Zahl, die, grob gesagt, bestimmt, \"wie leicht\" es ist, die Aktivierung dieses bestimmten Neurons zu verändern. Unser Input-Layer wir nicht gebiased sein.\n",
    "Wie genau berechnen jetzt sich die Aktivierungen der Neuronen des nächsten Layers aus den Aktivierungen des vorherigen Layers? Zuerst berechnen wir die Produkte aller Aktivierungen des vorherigen Layers und den Gewichtungen, die das Neuron, dem die Aktivierung gehört, mit dem Neuron, dessen Aktivierung wir berechnen wollen. Diese Produkte summieren wir auf. (Bis jetzt ist nicht mehr passiert als normale Matrix-Vektor-Multiplikation, wenn wir das vorherige Layer als Vektor und die Kanten zwischen den Layers als Übergangsmatrix interpretieren.) Zu dieser Summe addieren wir den Bias des Neurons, dessen Aktivierung wir berechnen wollen. Da wir wollen, dass die Aktivierung zwischen 0 und 1 liegt, wenden wir dann noch eine Funktion an, die das tut. Eine Funktion, die üblicherweise dafür benutzt wird, ist die Sigmoidfunktion:\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "  \n",
    "Die Sigmoidfunktion auf einen Vektor angewandt heißt nicht mehr, als sie auf jede Komponente des Vektors anzuwenden.\n",
    "Letztendlich ist die Formel für die Aktivierungen eines Layers l also:  \n",
    "  \n",
    "$$ a_l = \\sigma( W_{l-1->l} \\cdot a_{l-1} + b_l ) $$  \n",
    "  \n",
    "$ W_{l-1->l} $ ist hier die Übergangsmatrix zwischen Layer l-1 und Layer l, $a_{l-1}$ ist der Aktivierungsvektor des l-1-ten Layers und $b_l$ der Biasvektor des l-ten Layers.  \n",
    "Ein Neuron ist für uns jetzt nicht mehr nur etwas, das eine bestimmte Aktivierung hat, sondern eine Funktion, die alle Aktivierungen des vorherigen Layers in Abhängigkeit der Gewichtungen und einem Bias zu einer Zahl zwischen 0 und 1 verarbeitet. Und wirklich mehr als eine Funktion ist unser gesamtes neuronales Netz nicht! Der Input der Funktion ist ein Vektor von 784 Grauwerten, der dann Layer für Layer weiterverarbeitet wird, bis wir einen Output in der Form eines Aktivierungsvektors der Länge 10 bekommen. Diese Funktion ist natürlich abhängig von einer großen Menge an Parametern, nämlich den Gewichtungen und Biases der einzelnen Layer, aber mehr als eine solche Funktion ist unser Netz letztendlich nicht.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Der Lernprozess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wissen jetzt also, wie unser neuronales Netz einen Outputvektor in Abhängigkeit eines Pixelvektors und seiner Gewichtungen und Biases ausgibt. Doch wie können wir das Netz darauf trainieren, dass der Outputvektor so aussieht, wie wir gerne möchten, je nach dem, welche Ziffer wir auf dem Bild erkennen? Hier kommt die Idee von machine learning ins Spiel. Die Idee wird sein, dass wir dem Netz eine große Menge Trainingsbeispiele zeigen und es sie klassifizieren lassen. Anfangs wird diese Klassifikation natürlich noch sehr schlecht und ungenau ausfallen. Den resultierenden Output-Vektor vergleichen wir mit unserem erwarteten Output-Vektor und lassen das Netzwerk seine Gewichtungen uns Biases so anpassen, dass in Zukunft bei gleichem Input ein Output ausgegeben wird, der unserem erwarteten Output mehr ähnelt als der vorherige Output. Und hoffentlich, wenn wir dem Netz sehr viele Bilder zeigen, wird es irgendwann generelle Muster in den Bildern erkennen können, die es ihm erlauben, auch Bilder korrekt zu klassifizieren, die es noch nicht gesehen hat.  \n",
    "Zuerst initialisieren wir alle Gewichtungen und Biases zufällig. Natürlich wird der Output unseres Netzes (sehr wahrscheinlich) falsch sein. An dieser Stelle führen wir eine Funktion ein, die wir die Kostenfunktion des Netzes nennen. Sie soll uns, nachdem wir n Beispiele x getestet haben, ausgeben, \"wie schlecht\" das Netz arbeitet.\n",
    "$$ C(w, b) = \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2 $$\n",
    "w sind hier die Gewichtungen und b die Biases des Netzwerkes, x sind die Trainingsbeispiele und a ist der Output des Netzes. a hängt natürlich von w und b ab. y(x) ist der erwartete Output des Netzes, also wenn x das Bild einer 3 repräsentiert, ist $ y(x) = (0,0,0,1,0,0,0,0,0,0)^T $. $ \\|v\\| $ steht für die Länge des Vektors v. C nennen wir die * quadratische Kostenfunktion *.  \n",
    "Inwiefern macht es Sinn, diese Funktion so zu definieren? Zuerst einmal ist C nichtnegativ, das heißt C > 0, da jeder Summand nichtnegativ ist. Außerdem wird C groß sein, wenn y(x) und a in vielen der Trainingsbeispiele sehr unterschiedlich sind; und C wird klein, wenn y(x) und a oft ähnlich sind. Das Ziel unseres Lernprozesses wird es sein, C zu minimieren, das heißt, eine Konfiguration des Gewichtungen und Biases unseres Netzes zu finden, sodass C(w, b) möglichst klein ist.  \n",
    "Und das ist eigentlich alles, was hinter dem Lernprozess steckt! Wenn die Kostenfunktion des Netzwerkes gering wird, klassifiziert es die Trainingsbeispiele sehr wahrscheinlich ungefähr so, wie unser erwarteter Output ist. Dann können wir den Output des Netzes über eine Menge Testbeispiele, die kein Teil der Trainingsbeispiele waren, und beobachten, ob es \"gelernt\" hat, auch unbekannte Bilder zu klassifizieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastischer Gradientenabstieg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der nächste Schritt ist also, einen Algorithmus zu finden, der das Minimum der Kostenfunktion ermittelt. Wir werden das Minimum nicht explizit ermittlen; das ist zwar manchmal möglich, aber nicht immer geeignet, vor allem in einer so hochdimensionalen Funktion wie der Kostenfunktion. Was wir stattdessen machen werden, ist, zu schauen, \"in welche Richtung\" wir von unserem momentanen Punkt gehen müssen, damit die Funktion am schnellsten abnimmt. In dieser Richtung schauen wir wieder, in welche Richtung wir gehen müssen. Je öfter wir das machen, desto näher kommen wir an ein lokales Minimum der Funktion. Wenn wir dazu die Größe der Schritte proportional zur Stärke des Abstieges wählen, können wir auch nicht über das Minimum springen, sondern nähern uns von einer Seite immer weiter an. Das Bild, an das man dabei (in zwei oder drei Dimensionen) denken kann, ist ein Ball, der einen Berg herunter rollt. Das Bild ist zwar nicht hilfreich in so vielen Dimensionen, aber im Grunde ist die Idee die gleiche. Wir müssen wissen, wie wir die einzelnen Inputs der Funktion ändern müssen, damit der Output schnell kleiner wird. Nur ändern wir eben nicht eine oder zwei Koordinaten, sondern alle Gewichtungen und Biases.  \n",
    "Die mathematische Operation, die wir brauchen, ist der *Gradient* der Kostenfunktion. Der Gradient einer Funktion ist der Vektor des \"steilsten\" Anstiegs der Funktion an einem bestimmten Punkt, und - wenig überraschend - ist der negative Gradient der Vektor des \"steilsten\" Abstieges. Der negative Gradient der Kostenfunktion gibt uns also die Änderungen an den Gewichtungen uns Biases, die wir vornehmen müssen, um die Kostenfunktion möglichst schnell zu minimieren. Den ermitteln wir, indem wir das Netzwerk ein Trainingsbeispiel klassifizieren lassen, den tatsächlichen Output mithilfe der Kostenfunktion mit dem erwarteten Output vergleichen, den Gradienten der Kostenfunktion ermitteln und anschließend die Gewichtungen und Biases entsprechend anpassen.  \n",
    "Der Algorithmus, der uns diesen Gradienten liefert, nennt sich *Backpropagation*. Ich werde hier nicht weiter auf Backpropagation eingehen, für unsere Zwecke reicht es, zu wissen, dass es den Algorithmus gibt und er den Gradienten der Kostenfunktion unseres Netzes  ausgibt. Wen die Mathematik hinter der Backpropagation interessiert, kann sich im zweiten Kapitel von [Nielsen (2015)](http://neuralnetworksanddeeplearning.com/chap2.html) informieren.  \n",
    "Letztendlich werden wir jedoch nicht nach jedem Trainingsbeispiel einen Schritt nach unten machen, sondern den durchschnittlichen Gradienten von einer bestimmten Anzahl Trainingbeispiele, ein Mini-Batch, ermitteln, ohne zwischen der Ermittlung des Gradienten das Netzwerk anzupassen, und die Gewichtungen und Biases entsprechend dem durchschnittlichen Gradienten anpassen. Der entscheidende Vorteil dieses Verfahrens, stochastischer Gradientenabstieg genannt, ist, dass es in der programmierten Realisierung wesentlich schneller läuft als wenn man für jedes Beispiel alle Gewichtungen und Biases anpassen würde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ein paar notwendige Routinen vorweg ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die \"MNIST database of handwritten digits\" ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die [MNIST-Datenbank](http://yann.lecun.com/exdb/mnist/) ist eine von Yann LeCun, Corinna Cortes und Christopher J.C. Burges. zusammengestellte Datenbank mit 70,000 Schwarz-Weiß-Bildern von handgeschriebenen Ziffern mit dazugehörigen Labels, aufgeteilt in ein Trainingset mit 60,000 und ein Testset mit 10,000 Bildern und Labels.\n",
    "Die Ziffern in den einzelnen Sets stammen ungefähr zur Hälfte von Highschool-Schülern und zur Hälfte von MitarbeiterInnen des United States Census Bureaus. Geammelt wurden die Daten vom Unites States National Institute of Standards and Technology (NIST) und in zwei Sets, Special Database 1 (mit den von den Highschool-Schülern geschriebenen Ziffern) und Special Database 3 (mit den von den MitarbeiterInnen des Census Bureaus geschriebenen Ziffern) zusammengefasst. \n",
    "Mithilfe des Codes unten deklarieren wir vier Arrays: x_train und x_test beinhalten die 60,000 bzw. 10,000 Bilder aus dem Trainings- bzw. Testset. Konkret ist jeder Eintrag dieser Arrays ein Array mit 784 Integern von 0 bis 255. Jeder Eintrag repräsentiert den Grauwert eines Pixels des Bildes. t_train und t_test beinhalten die zu den Bildern gehörigen Zahlen als Integer. x_train[0] zum Beispiel ist das Bild einer 5, also ist t_train[0] = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "mnist.init()\n",
    "x_train, t_train, x_test, t_test = mnist.load() # Initialisierung der Trainings- und Test-Array-Variablen wie oben beschrieben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst einmal wollen wir eine Möglichkeit haben, uns unsere Zahlen anzeigen zu lassen. Wir benutzen dafür imshow aus matplotlib.pyplot.\n",
    "imshow benötigt aber nicht ein Array von der Länge 784 (wie die Arrays in x_train und x_test), sondern ein 28x28-Array mit den Pixelwerten. Die Konvertierung übernimmt die Funktion *bildconv*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bildconv(bild):\n",
    "    pixellist = []                                      # Wir erstellen eine leere Liste \"pixellist\".\n",
    "    for i in range(28):                                 \n",
    "        pixellist.append([])                            # Wir erstellen 28 Zeilen\n",
    "        for j in range(28):\n",
    "            pixellist[i].append(bild[i*28 + j])         # und füllen diese mit je 28 Einträgen aus dem ursprünglichen Array.\n",
    "    return pixellist                                    # 28x28-array bestehend aus 28 Zeilen mit 28 Werten/Spalten pro Zeile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den Output dieser Funktion können wir als Eingabewert für imshow aus matplotlib verwenden und mit dem dazugehörigen Wert aus t_train vergleichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In t_train steht, dass das ausgewählte Beispiel aus x_train eine 4 ist.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAADP9JREFUeJzt3VGIXPXZx/HfL9qA2CqJtctigklLFIpEW1apVjQlGtJQiL1QGrSmVLKCFVroRcVeVJCCFtvSGwtbDYmveW1fiKuh1NemoWgLGnYjVk1iEhsSu0tMKlaaothGn17Mid3GnTObmTNzZvf5fmDZmfPMmXk47G//58w5M39HhADkM6/uBgDUg/ADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0jqzF6+mG0uJwS6LCI8k8d1NPLbXm17n+3XbN/VyXMB6C23e22/7TMk7Zd0vaQJSWOS1kXEnpJ1GPmBLuvFyH+FpNci4mBE/FPSLyWt7eD5APRQJ+G/QNJfptyfKJb9F9vDtsdtj3fwWgAq1vU3/CJiRNKIxG4/0E86GfknJS2ecn9RsQzALNBJ+MckLbO91PZ8SV+TtK2atgB0W9u7/RFxwvadkp6WdIakjRGxu7LOAHRV26f62noxjvmBruvJRT4AZi/CDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkmp7im5Jsn1I0nFJ70s6ERFDVTQFVGHlypVNa1u2bCld99prry2t79u3r62e+klH4S98KSLerOB5APQQu/1AUp2GPyT91vYu28NVNASgNzrd7b86IiZtf0rSdtuvRsSzUx9Q/FPgHwPQZzoa+SNisvh9TNKopCumecxIRAzxZiDQX9oOv+2zbX/i5G1JqyS9UlVjALqrk93+AUmjtk8+z/9GxP9X0hWArms7/BFxUNKlFfbSVddcc01p/bzzziutj46OVtkOeuDyyy9vWhsbG+thJ/2JU31AUoQfSIrwA0kRfiApwg8kRfiBpKr4VN+ssGLFitL6smXLSuuc6us/8+aVj11Lly5tWrvwwgtL1y2uX5nTGPmBpAg/kBThB5Ii/EBShB9IivADSRF+IKk05/lvvfXW0vpzzz3Xo05QlcHBwdL6hg0bmtYeffTR0nVfffXVtnqaTRj5gaQIP5AU4QeSIvxAUoQfSIrwA0kRfiCpNOf5W332G7PPQw891Pa6Bw4cqLCT2YlEAEkRfiApwg8kRfiBpAg/kBThB5Ii/EBSLc/z294o6SuSjkXEJcWyhZJ+JWmJpEOSboqIv3WvzdaWL19eWh8YGOhRJ+iVc889t+11t2/fXmEns9NMRv5NklafsuwuSTsiYpmkHcV9ALNIy/BHxLOS3jpl8VpJm4vbmyXdUHFfALqs3WP+gYg4Utx+QxL71MAs0/G1/RERtqNZ3fawpOFOXwdAtdod+Y/aHpSk4vexZg+MiJGIGIqIoTZfC0AXtBv+bZLWF7fXS3qymnYA9ErL8Nt+TNJzki62PWH7Nkn3Sbre9gFJ1xX3AcwiLY/5I2Jdk9LKinvpyJo1a0rrZ511Vo86QVVaXZuxdOnStp97cnKy7XXnCq7wA5Ii/EBShB9IivADSRF+ICnCDyQ1Z766++KLL+5o/d27d1fUCarywAMPlNZbnQrcv39/09rx48fb6mkuYeQHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaTmzHn+To2NjdXdwqx0zjnnlNZXrz71i5//45Zbbildd9WqVW31dNK9997btPb222939NxzASM/kBThB5Ii/EBShB9IivADSRF+ICnCDyTFef7CwoULa3vtSy+9tLRuu7R+3XXXNa0tWrSodN358+eX1m+++ebS+rx55ePHu+++27S2c+fO0nXfe++90vqZZ5b/+e7atau0nh0jP5AU4QeSIvxAUoQfSIrwA0kRfiApwg8k5Ygof4C9UdJXJB2LiEuKZfdI2iDpr8XD7o6I37R8Mbv8xTrw4IMPltZvv/320nqrz3e//vrrp93TTC1fvry03uo8/4kTJ5rW3nnnndJ19+zZU1pvdS5+fHy8tP7MM880rR09erR03YmJidL6ggULSuutrmGYqyKi/A+mMJORf5Ok6b6R4acRcVnx0zL4APpLy/BHxLOS3upBLwB6qJNj/jttv2R7o+3y/S8Afafd8P9c0mckXSbpiKQfN3ug7WHb47bLDw4B9FRb4Y+IoxHxfkR8IOkXkq4oeexIRAxFxFC7TQKoXlvhtz045e5XJb1STTsAeqXlR3ptPyZphaRP2p6Q9ANJK2xfJikkHZJUfh4NQN9pGf6IWDfN4oe70EtH7rjjjtL64cOHS+tXXXVVle2cllbXEDzxxBOl9b179zatPf/882311AvDw8Ol9fPPP7+0fvDgwSrbSYcr/ICkCD+QFOEHkiL8QFKEH0iK8ANJpfnq7vvvv7/uFnCKlStXdrT+1q1bK+okJ0Z+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0gqzXl+zD2jo6N1tzCrMfIDSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUi0/z297saRHJA1ICkkjEfEz2wsl/UrSEkmHJN0UEX/rXqvIxnZp/aKLLiqt9/P05P1gJiP/CUnfjYjPSvqCpG/Z/qykuyTtiIhlknYU9wHMEi3DHxFHIuKF4vZxSXslXSBpraTNxcM2S7qhW00CqN5pHfPbXiLpc5J2ShqIiCNF6Q01DgsAzBIz/g4/2x+XtFXSdyLi71OPxyIibEeT9YYlDXfaKIBqzWjkt/0xNYK/JSIeLxYftT1Y1AclHZtu3YgYiYihiBiqomEA1WgZfjeG+Icl7Y2In0wpbZO0vri9XtKT1bcHoFtmstv/RUlfl/Sy7ReLZXdLuk/S/9m+TdJhSTd1p0VkFTHtkeSH5s3jMpVOtAx/RPxRUrMTrp1NsA6gNvzrBJIi/EBShB9IivADSRF+ICnCDyTFFN2Yta688srS+qZNm3rTyCzFyA8kRfiBpAg/kBThB5Ii/EBShB9IivADSXGeH32r1Vd3ozOM/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOf5UZunnnqqtH7jjTf2qJOcGPmBpAg/kBThB5Ii/EBShB9IivADSRF+ICm3mgPd9mJJj0gakBSSRiLiZ7bvkbRB0l+Lh94dEb9p8VzlLwagYxExoy9CmEn4ByUNRsQLtj8haZekGyTdJOkfEfHATJsi/ED3zTT8La/wi4gjko4Ut4/b3ivpgs7aA1C30zrmt71E0uck7SwW3Wn7JdsbbS9oss6w7XHb4x11CqBSLXf7P3yg/XFJz0j6YUQ8bntA0ptqvA9wrxqHBt9s8Rzs9gNdVtkxvyTZ/pikX0t6OiJ+Mk19iaRfR8QlLZ6H8ANdNtPwt9ztd+MrVB+WtHdq8Is3Ak/6qqRXTrdJAPWZybv9V0v6g6SXJX1QLL5b0jpJl6mx239I0u3Fm4Nlz8XID3RZpbv9VSH8QPdVttsPYG4i/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK8ANJEX4gKcIPJNXrKbrflHR4yv1PFsv6Ub/21q99SfTWrip7u3CmD+zp5/k/8uL2eEQM1dZAiX7trV/7kuitXXX1xm4/kBThB5KqO/wjNb9+mX7trV/7kuitXbX0VusxP4D61D3yA6hJLeG3vdr2Ptuv2b6rjh6asX3I9su2X6x7irFiGrRjtl+Zsmyh7e22DxS/p50mrabe7rE9WWy7F22vqam3xbZ/b3uP7d22v10sr3XblfRVy3br+W6/7TMk7Zd0vaQJSWOS1kXEnp420oTtQ5KGIqL2c8K2r5H0D0mPnJwNyfaPJL0VEfcV/zgXRMT3+qS3e3SaMzd3qbdmM0t/QzVuuypnvK5CHSP/FZJei4iDEfFPSb+UtLaGPvpeRDwr6a1TFq+VtLm4vVmNP56ea9JbX4iIIxHxQnH7uKSTM0vXuu1K+qpFHeG/QNJfptyfUH9N+R2Sfmt7l+3hupuZxsCUmZHekDRQZzPTaDlzcy+dMrN032y7dma8rhpv+H3U1RHxeUlflvStYve2L0XjmK2fTtf8XNJn1JjG7YikH9fZTDGz9FZJ34mIv0+t1bntpumrlu1WR/gnJS2ecn9RsawvRMRk8fuYpFE1DlP6ydGTk6QWv4/V3M+HIuJoRLwfER9I+oVq3HbFzNJbJW2JiMeLxbVvu+n6qmu71RH+MUnLbC+1PV/S1yRtq6GPj7B9dvFGjGyfLWmV+m/24W2S1he310t6ssZe/ku/zNzcbGZp1bzt+m7G64jo+Y+kNWq84/9nSd+vo4cmfX1a0p+Kn9119ybpMTV2A/+lxnsjt0k6T9IOSQck/U7Swj7q7X/UmM35JTWCNlhTb1ersUv/kqQXi581dW+7kr5q2W5c4QckxRt+QFKEH0iK8ANJEX4gKcIPJEX4gaQIP5AU4QeS+jePVgFoos9YrwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imnumber = 2 # imnumber wird genutzt, um ein Beispiel aus dem Set x_train auszuwählen, dem an entsprechender Stelle in t_train die geschriebene Ganzzahl korrelliert.\n",
    "    \n",
    "imgplot = plt.imshow(bildconv(x_train[int(imnumber)]), cmap=\"gray\")\n",
    "print(\"In t_train steht, dass das ausgewählte Beispiel aus x_train eine\",t_train[int(imnumber)],\"ist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes brauchen wir eine Funktion, die den erwarteten Output-Vektor des Netzes in Abhängigkeit der erwarteten Zahl ausgibt. In t_train und t_test stehen ja Integer. Unser Netz wird aber einen Vektor der Größe 10 haben, wobei der n-te Eintrag die Zahl n repräsentiert. Um die Einträge in t_train und t_test also mit den Outputs unseres Netzes vergleichen zu können, müssen wir einen Vektor der Größe 10 erstellen, dessen Einträge v_t für $t\\neq n$  0 sind und für $t = n$ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    temp = np.zeros(10)\n",
    "    temp[j] = 1.0\n",
    "    return temp # 10-Komponenten-Vektor mit dem Wert 1 an der Stelle j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noch zwei notwendige Funktionen: Die Sigmoidfunktion, die wir anstatt einem Threshold benutzen werden, um die Aktivierungen der Neuronen zwischen 0 und 1 zu halten, und ihre Ableitung, die für die Backpropagation benötigt wird. Wir benutzen also keine binären Aktivierungen.\n",
    "\n",
    "$$ Sigmoid(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "Da exp(-x) für kleine x sehr schnell sehr groß wird und Overflow in Python verursacht, benutzen wir eine andere Schreibweise der Sigmoidfunktion. Durch Erweitern des Bruches mit $ e^x $ erhalten wir:\n",
    "\n",
    "$$ Sigmoid(x) = \\frac{e^x}{e^x+1} $$\n",
    "\n",
    "Für kleine x ist die untere Schreibweise numerisch stabiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):                             # Die Sigmoidfunktion, angewandt auf jeden Eintrag eines Vektors\n",
    "    temp = []\n",
    "    for i in z:\n",
    "        temp.append(sigmoidint(i))\n",
    "    return temp\n",
    "def sigmoidint(z):                          # Die Sigmoidfunktion, angewandt auf eine einzelnen Integer\n",
    "    if z >= 0:\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    else:\n",
    "        return np.exp(z) / (1 + np.exp(z))\n",
    "\n",
    "def sigmoid_prime(z):                       # Die erste Ableitung der Sigmoidfunktion, wie oben\n",
    "    temp = []\n",
    "    for i in z:\n",
    "        temp.append(sigmoidint(i)*(1-sigmoidint(i)))\n",
    "    return temp\n",
    "\n",
    "def sigmoid_primeint(z):\n",
    "    return sigmoidint(z)*(1-sigmoidint(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Die Funktionen des Netzwerkes</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir werden das Netz als Klasse *Network* aufbauen. Im Folgenden werde ich die Methoden der Klasse näher erklären.\n",
    "\n",
    "#### *feedforward(self, a)*\n",
    "Diese Funktion gibt den Output des Netzes in Abhängigkeit eines Inputs a aus. Wir iterieren über alle weights und biases und speichern mit jeder Iteration a die Aktivierungen des jeweils nächsten Layers. Vor der ersten Iteration repräsentiert a einfach nur das Input-Layer, nach der ersten Itreation repräsentiert a das zweite Layer, und so weiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *cost_derivative(self, output_activations, y)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir benutzen die quadratische Kostenfunktion\n",
    "\n",
    "C_x(w, b) = ( ||(y(x)) - a|| ^2 ) / 2 ,\n",
    "\n",
    "wobei x ein Input ist, y(x) der erwartete Output und a der tatsächliche Output. (Dieser tatsächliche Output hängt natürlich von den\n",
    "weights und biases des Netzwerkes, sowie vom Input x ab.)\n",
    "Diese Kostenfunktion bestimmt den Fehler des Netzwerkes für einen einzelnen Input. Für die Kostfunktion des gesamten Netzwerkes würde man den Durchschnitt\n",
    "von C_x über n Inputs berechnen. \n",
    "Die Kostenfunktion selbst brauchen wir nicht, wir benötigen für die Backpropagation nur ihre Ableitung."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt zu dem Code für das Netzwerk selbst:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        \"\"\"Das Array sizes besteht aus den Größen der einzelnen Layer unseres Netzwerkes. In unserem Fall wäre sizes also [784, n, 10], wobei n die Größe unseres Hidden Layers\n",
    "        repräsentiert. Im Folgenden werden die weights und biases zufällig gesetzt. Das erste Layer bekommt keine biases, da wir es als Input-Layer benutzen.\n",
    "        Die biases werden also nur benutzt, um die Aktivierungen der späteren Layer zu berechnen.\"\"\"\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(i) for i in sizes[1:]]       #biases ist jetzt eine Liste von Arrays, die die (zufällig gesetzten) biases pro Layer repräsentieren.\n",
    "                                                                    #Die Länge von biases entspricht der Länge von sizes, die Länge jedes einzelnen Arrays in biases entspricht\n",
    "                                                                    #dem entsprechenden Eintrag in sizes.\n",
    "        self.weights = [np.random.randn(i, j) for j, i in zip(sizes[:-1], sizes[1:])]  \n",
    "                                                                    #weights ist eine Liste, in der die einzelnen Matrizen gespeichert werden, die die Layers miteinander\n",
    "                                                                    #verbinden. Bei weights[1] findet sich also die Matrix, die das zweite mit dem dritten layer verbindet.\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.add(np.dot(w, a),b))\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta):\n",
    "        \n",
    "        n = len(training_data)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            print(\"Epoche\",j+1,\"beendet.\")\n",
    "    \"\"\"Wir wenden Stochastic Gradient Descent auf das Netzwerk an.\"\"\"\n",
    "                \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "                                                                                #Diese Zeile macht die meiste Arbeit. Der Backpropagation-Algorithmus wird unten definiert\n",
    "                                                                                #und erklärt. Kurz gesagt gibt er die Ableitung der Kostenfunktion an der Stelle, an der sich das\n",
    "                                                                                #Netzwerk momentan befindet. Der Output ist ein Tupel mit den einzelnen Werten der weights und\n",
    "                                                                                #biases, mit der gleichen Formatierung wie weights und biases. Mithilfe der Ableitung können wir\n",
    "                                                                                #die Kostenfunktion nach und nach minimieren.\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "                                                                                #In nabla_b und nabla_w speichern wir die durchschnittlichen Änderungen, die wir an\n",
    "                                                                                #weights und biases vornehmen müssen, die wir mithilfe des Mini-Batches berechnen.\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "                                                                                #Dann passen wir die weights und biases entsprechend an.\n",
    "            \n",
    "            \n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "                                                                                #Diese Funktion lässt das Netzwerk die test_data durchgehen und\n",
    "                                                                                #gibt die Anzahl der richtigen Antworten des Netzwerkes aus.\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Diese Funktion gibt den Gradienten der Kostenfunktion in Bezug auf die weights und biases aus. Die Ausgabe findet als Tupel (nabla_b, nabla_w)\n",
    "        statt, in der gleichen Form wie weights und biases. So können wir den Gradienten in update_mini_batch intuitiv verwenden.\"\"\"\n",
    "        deltas = [np.zeros(i) for i in self.sizes[0:]] # deltas speichert die Fehler, den das Netz pro Layer produziert.\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # In activations speichern wir die Aktivierungen der einzelnen Layer.\n",
    "        zs = [] # In zs speichern wir die einzelnen Layer, bevor die Sigmoid-Funktion angewandt wird.\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        deltas[-1] = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            deltas[-l] = np.dot(np.transpose(self.weights[-l+1]), deltas[-l+1]) * sp\n",
    "\n",
    "        for l in range(len(nabla_w)):\n",
    "            nabla_w[l] = np.outer(deltas[l+1], np.asarray(activations[l]).transpose())\n",
    "            \n",
    "        return (deltas[1:], nabla_w)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "test_data = []\n",
    "\n",
    "for i, j in zip(x_train, t_train):\n",
    "    training_data.append((i/255,vectorized_result(j)))\n",
    "    \n",
    "for i, j in zip(x_test, t_test):\n",
    "    test_data.append((i/255,vectorized_result(j)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9727716303123116, 0.2647018929489321, 0.0024041746618560836, 0.63005130294358, 0.02663027746162897, 0.1456827626751845, 0.23303894534182654, 0.001644230579216315, 0.5944262912638074, 0.08783202254777694]\n",
      "Ich bin mir zu 97.27716303123117 % sicher, dass das eine 0 ist.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = Network([784, 20, 10])\n",
    "example = 1\n",
    "outp = net.feedforward(x_train[example])\n",
    "print(outp)\n",
    "print(\"Ich bin mir zu\", np.amax(outp)*100,\"% sicher, dass das eine\",np.argmax(outp),\"ist.\")\n",
    "imgplot = plt.imshow(bildconv(x_train[example]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versuchen wir nun, das Netzwerk mithilfe der MNIST-Training-Data zu trainieren. Versuchen wir es mit einem Hidden Layer mit 20 Neuronen, 30 Epochen, einer Mini-Batch-Größe von 10 und einer Lernrate eta = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.SGD(training_data, epochs = 30, mini_batch_size = 10, eta = 3.0)\n",
    "\n",
    "evaluation = net.evaluate(test_data)\n",
    "\n",
    "print(\"Ich habe von\",len(test_data),\"Beispielen im Test-Set\",evaluation,\"richtig klassifiziert. Das entspricht einer Genauigkeit von\",(evaluation/len(test_data))*100,\"%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = 9888\n",
    "outp = net.feedforward(x_test[example])\n",
    "print(outp)\n",
    "print(\"Ich bin mir zu\", np.amax(outp)*100,\"% sicher, dass das eine\",np.argmax(outp),\"ist.\")\n",
    "imgplot = plt.imshow(bildconv(x_test[example]), cmap=\"gray\")\n",
    "print(\"Eigentlich ist es eine\",t_test[example],\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example_array = np.random.randn(784)\n",
    "outp = net.feedforward(example_array)\n",
    "print(outp)\n",
    "print(\"Ich bin mir zu\", np.amax(outp)*100,\"% sicher, dass das eine\",np.argmax(outp),\"ist.\")\n",
    "imgplot = plt.imshow(bildconv(example_array), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net2 = Network([784, 300, 200, 10])\n",
    "\n",
    "net2.SGD(training_data, epochs = 2, mini_batch_size = 10, eta = 3.0)\n",
    "\n",
    "evaluation = net2.evaluate(test_data)\n",
    "\n",
    "print(\"Ich habe von\",len(test_data),\"Beispielen im Test-Set\",evaluation,\"richtig klassifiziert. Das entspricht einer Genauigkeit von\",(evaluation/len(test_data))*100,\"%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = 9888\n",
    "outp = net2.feedforward(x_test[example])\n",
    "print(outp)\n",
    "print(\"Ich bin mir zu\", np.amax(outp)*100,\"% sicher, dass das eine\",np.argmax(outp),\"ist.\")\n",
    "imgplot = plt.imshow(bildconv(x_test[example]), cmap=\"gray\")\n",
    "print(\"Eigentlich ist es eine\",t_test[example],\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Litaratuar ##\n",
    "\n",
    "MNIST-Seite: http://yann.lecun.com/exdb/mnist/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3pack",
   "language": "python",
   "name": "py3pack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
