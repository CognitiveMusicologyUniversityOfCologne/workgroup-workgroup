{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eine konzeptionelle Einführung in neuronale Netze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Text soll eine konzeptionelle Einführung in neuronale Netze bieten. Nach der generellen Erklärung des Konzeptes wird ein einfaches Netz anhand eines Python-Codes programmiert.\n",
    "Als Arbeitsbeispiel dient hier ein Netz zur Erkennung handgeschriebener Ziffern. Diese Aufgabe ist das Standardanwendungsbeispiel für die Einführung in machine learning. Sie ist einerseits sehr intuitiv, andererseits sehr komplex. Für uns ist Ziffernerkennung keine schwere Aufgabe. Unser Gehirn bewältigt sie mehrmals am Tag so problemlos, dass wir meistens nicht mal etwas davon merken. Trotzdem können wir nicht sagen, wie *genau* wir diese Erkennung schaffen; und es wäre vermutlich für die meisten Menschen unmöglich, diesen Prozess in einem Algorithmus zu konkretisieren. Trotzdem kann sogar ein \"basales\" neuronales Netz diese Aufgabe gut bewältigen. Ein weiterer Vorteil dieses Beispieles ist, dass bereits ein einfach benutzbarer Datensatz für Trainings- und Testmengen existiert, die MNIST database of handwritten digits. Sie beinhaltet 70,000 Bilder handgeschriebener Ziffern. Die einzelnen Bilder sind 28x28 Pixel groß und monochrom. Wir werden sehen, dass keine aufwendige Umstrukturierung der Daten notwendig ist, um sie als Input für unser neuronales Netz zu benutzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die allgemeine Idee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die Netzwerkstruktur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Idee, die hinter neuronalen Netzen steht, ist, einen Algorithmus zu produzieren, der aus einer großen Datenmenge, den Trainingsbeispielen, lernt. Wir hoffen, dass unser neuronales Netz, wenn wir ihm - in unserem Fall - 60,000 verschiedene Bilder von handgeschriebenen Ziffern zeigen, Muster erkennt und weitere Bilder handgeschriebener Ziffern klassifizieren kann.  \n",
    "Der kleinste Bestandteil eines neuronalen Netzes ist, wie der Name nahelegt, ein Neuron. Erst einmal reicht es, sich ein Neuron als etwas vorzustellen, dass eine Zahl speichert - konkret eine Zahl zwischen 0 und 1. Diese Zahl nennen wir die Aktivierung des Neurons.  \n",
    "Die einzelnen Neuronen des Netzwerkes sind in sogenannten Layern sortiert, sozusagen Vektoren von Neuronen. Das erste Layer ist das Input-Layer. In unserem Fall wird es 784 Neuronen enthalten, die die einzelnen Grauwerte der Pixel des zu klassifizierenden Bildes repräsentieren. Das letzte Layer, das Output-Layer, soll 10 Neuronen enthalten, von denen jeder repräsentiert, wie sicher sich das Netzwerk ist, dass es sich bei dem Bild im Input-Layer um eine bestimmte Zahl handelt. Unsere Hoffnung ist, dass, wenn wir das Bild einer 3 in das Input-Layer speisen (und das Netzwerk genug gelernt hat), die Aktivierung des vierten (vierten und nicht dritten, weil der erste Eintrag für 0 steht) Neurons im Output-Layer hoch ist und die Aktivierung aller anderen Neuronen niedrig.\n",
    "Zwischen dem Input- und dem Output-Layer liegen noch sogenannte Hidden Layer. Ihre Größe und Anzahl können wir selbst wählen und wir werden versuchen, ein wenig mit ihnen herumzuprobieren und beobachten, wie sich Änderungen dort auf die Effizienz des Netzwerkes auswirken.  \n",
    "Die Aktivierungen eines Layers sollen die Aktivierungen des nächsten Layers beeinflussen. Dafür wird jedes Layer mit dem darauf folgenden durch Kanten verbunden, jedes Neuron im ersten hat eine Kante zu jedem Neuron im zweiten Layer, dort wiederum hat jedes Neuron eine Kante zu jedem Neuron im dritten Layer und so weiter bis zum Output-Layer. Die Kanten sind nicht mehr als Zahlen, Gewichtungen oder weights genannt, die determinieren, wie die Aktivierungen in einem Layer aussehen, und zwar in Abhängigkeit der Aktivierungen des vorherigen Layers. Wir können uns die Kanten zwischen zwei Layern als eine Übergangsmatrix vom vorherigen zum nächsten Layer. Tatsächlich werden die Aktivierungen des nächsten Layers im Grunde so berechnet.  \n",
    "Außerdem bekommt jedes Neuron in den Layern hinter dem Input-Layer einen Bias, eine Zahl, die, grob gesagt, bestimmt, \"wie leicht\" es ist, die Aktivierung dieses bestimmten Neurons zu verändern. Unser Input-Layer wir nicht gebiased sein, da der Input möglichst unverändert verarbeitet werden soll.\n",
    "Wie genau berechnet jetzt sich die Aktivierung jedes Neurons des nächsten Layers aus den Aktivierungen des vorherigen Layers? Zuerst berechnen wir die Produkte aller Aktivierungen des vorherigen Layers und den Gewichtungen, die das Neuron, dem die Aktivierung gehört, mit dem Neuron, dessen Aktivierung wir berechnen wollen. Diese Produkte summieren wir auf. (Bis jetzt ist nicht mehr passiert als normale Matrix-Vektor-Multiplikation, wenn wir das vorherige Layer als Vektor und die Kanten zwischen den Layers als Übergangsmatrix interpretieren.) Zu dieser Summe addieren wir den Bias des Neurons, dessen Aktivierung wir berechnen wollen. Da wir wollen, dass die Aktivierung zwischen 0 und 1 liegt, wenden wir dann noch eine Funktion an, die das tut. Eine Funktion, die üblicherweise dafür benutzt wird, ist die Sigmoidfunktion:\n",
    "$$ \\sigma(x) = \\frac{1}{1 + e^{-x}}. $$\n",
    "  \n",
    "Die Sigmoidfunktion auf einen Vektor angewandt heißt nicht mehr, als sie auf jede Komponente des Vektors anzuwenden.\n",
    "Letztendlich ist die Formel für die Aktivierungen eines Layers l also:  \n",
    "  \n",
    "$$ a_l = \\sigma( W_{l-1->l} \\cdot a_{l-1} + b_l ). $$  \n",
    "  \n",
    " $ W_{l-1->l} $ ist hier die Übergangsmatrix zwischen Layer l-1 und Layer l, $a_{l-1}$ ist der Aktivierungsvektor des l-1-ten Layers und $b_l$ der Biasvektor des l-ten Layers.  \n",
    "Ein Neuron ist für uns jetzt nicht mehr nur etwas, das eine bestimmte Aktivierung hat, sondern eine Funktion, die alle Aktivierungen des vorherigen Layers in Abhängigkeit der Gewichtungen und einem Bias zu einer Zahl zwischen 0 und 1 verarbeitet. Und wirklich mehr als eine Funktion ist unser gesamtes neuronales Netz nicht! Der Input der Funktion ist ein Vektor von 784 Grauwerten, der dann Layer für Layer weiterverarbeitet wird, bis wir einen Output in der Form eines Aktivierungsvektors der Länge 10 bekommen. Diese Funktion ist natürlich abhängig von einer großen Menge an Parametern, nämlich den Gewichtungen und Biases der einzelnen Layer, aber mehr als eine solche Funktion ist unser Netz letztendlich nicht.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Der Lernprozess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir wissen jetzt also, wie unser neuronales Netz einen Outputvektor in Abhängigkeit eines Pixelvektors und seiner Gewichtungen und Biases ausgibt. Doch wie können wir das Netz darauf trainieren, dass der Outputvektor so aussieht, wie wir gerne möchten, je nach dem, welche Ziffer wir auf dem Bild erkennen? Hier kommt die Idee von machine learning ins Spiel. Die Idee wird sein, dass wir dem Netz eine große Menge Trainingsbeispiele zeigen und es sie klassifizieren lassen. Anfangs wird diese Klassifikation natürlich noch sehr schlecht und ungenau ausfallen. Den resultierenden Output-Vektor vergleichen wir mit unserem erwarteten Output-Vektor und lassen das Netzwerk seine Gewichtungen uns Biases so anpassen, dass in Zukunft bei gleichem Input ein Output ausgegeben wird, der unserem erwarteten Output mehr ähnelt als der vorherige Output. Und hoffentlich, wenn wir dem Netz sehr viele Bilder zeigen, wird es irgendwann generelle Muster in den Bildern erkennen können, die es ihm erlauben, auch Bilder korrekt zu klassifizieren, die es noch nicht gesehen hat.  \n",
    "Zuerst initialisieren wir alle Gewichtungen und Biases zufällig. Natürlich wird der Output unseres Netzes (sehr wahrscheinlich) sehr schlecht sein. An dieser Stelle führen wir eine Funktion ein, die wir die Kostenfunktion des Netzes nennen. Sie soll uns, nachdem wir n Beispiele x getestet haben, ausgeben, \"wie schlecht\" das Netz arbeitet.\n",
    "$$ C(w, b) = \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2. $$\n",
    "w sind hier die Gewichtungen und b die Biases des Netzwerkes, x sind die Trainingsbeispiele und a ist der Output des Netzes. a hängt natürlich von w und b ab. y(x) ist der erwartete Output des Netzes, also wenn x das Bild einer 3 repräsentiert, ist $ y(x) = (0,0,0,1,0,0,0,0,0,0)^T $. $ \\|v\\| $ steht für die Länge des Vektors v. C nennen wir die * quadratische Kostenfunktion *.  \n",
    "Inwiefern macht es Sinn, diese Funktion so zu definieren? Zuerst einmal ist C nichtnegativ, das heißt C > 0, da jeder Summand nichtnegativ ist. Außerdem wird C groß sein, wenn y(x) und a in vielen der Trainingsbeispiele sehr unterschiedlich sind; und C wird klein, wenn y(x) und a oft ähnlich sind. Das Ziel unseres Lernprozesses wird es sein, C zu minimieren, das heißt, eine Konfiguration des Gewichtungen und Biases unseres Netzes zu finden, sodass C(w, b) möglichst klein ist.  \n",
    "Und das ist eigentlich alles, was hinter dem Lernprozess steckt! Wenn die Kostenfunktion des Netzwerkes gering wird, klassifiziert es die Trainingsbeispiele sehr wahrscheinlich ungefähr so, wie unser erwarteter Output ist. Dann können wir den Output des Netzes über eine Menge Testbeispiele, die kein Teil der Trainingsbeispiele waren, und beobachten, ob es \"gelernt\" hat, auch unbekannte Bilder zu klassifizieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastischer Gradientenabstieg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der nächste Schritt ist also, einen Algorithmus zu finden, der das Minimum der Kostenfunktion ermittelt. Wir werden das Minimum nicht explizit ermittlen; das ist zwar manchmal möglich, aber nicht immer geeignet, vor allem in einer so hochdimensionalen Funktion wie der Kostenfunktion (Der Input sind ja alle Gewichtungen und Biases des Netzes). Was wir stattdessen machen werden, ist, zu schauen, \"in welche Richtung\" wir von unserem momentanen Punkt gehen müssen, damit die Funktion am schnellsten abnimmt. In dieser Richtung schauen wir wieder, in welche Richtung wir gehen müssen. Je öfter wir das machen, desto näher kommen wir an ein lokales Minimum der Funktion. Wenn wir dazu die Größe der Schritte proportional zur Stärke des Abstieges wählen, können wir auch nicht über das Minimum springen, sondern nähern uns von einer Seite immer weiter an. Das Bild, an das man dabei (in zwei oder drei Dimensionen) denken kann, ist ein Ball, der einen Berg herunter rollt. Das Bild ist zwar nicht hilfreich in so vielen Dimensionen, aber im Grunde ist die Idee die gleiche. Wir müssen wissen, wie wir die einzelnen Inputs der Funktion ändern müssen, damit der Output schnell kleiner wird. Nur ändern wir eben nicht eine oder zwei Koordinaten, sondern alle Gewichtungen und Biases.  \n",
    "Die mathematische Operation, die wir brauchen, ist der *Gradient*. Der Gradient einer Funktion ist der Vektor des \"steilsten\" Anstiegs der Funktion an einem bestimmten Punkt, und - wenig überraschend - ist der negative Gradient der Vektor des \"steilsten\" Abstieges. Der negative Gradient der Kostenfunktion gibt uns also die Änderungen an den Gewichtungen uns Biases, die wir vornehmen müssen, um die Kostenfunktion möglichst schnell zu minimieren. Den ermitteln wir, indem wir das Netzwerk ein Trainingsbeispiel klassifizieren lassen, den tatsächlichen Output mithilfe der Kostenfunktion mit dem erwarteten Output vergleichen, den Gradienten der Kostenfunktion ermitteln und anschließend die Gewichtungen und Biases entsprechend anpassen.  \n",
    "Der Algorithmus, der uns diesen Gradienten liefert, nennt sich *Backpropagation*. Ich werde hier nicht weiter auf Backpropagation eingehen, für unsere Zwecke reicht es, zu wissen, dass es den Algorithmus gibt und er den Gradienten der Kostenfunktion unseres Netzes  ausgibt. Wen die Mathematik hinter der Backpropagation interessiert, kann sich im zweiten Kapitel von [Nielsen (2015)](http://neuralnetworksanddeeplearning.com/chap2.html) informieren.  \n",
    "Letztendlich werden wir jedoch nicht nach jedem Trainingsbeispiel einen Schritt nach unten machen, sondern den durchschnittlichen Gradienten von einer bestimmten Anzahl Trainingbeispiele, ein Mini-Batch, ermitteln, ohne zwischen der Ermittlung des Gradienten das Netzwerk anzupassen, und die Gewichtungen und Biases entsprechend dem durchschnittlichen Gradienten anpassen. Der entscheidende Vorteil dieses Verfahrens, stochastischer Gradientenabstieg genannt, ist, dass es in der programmierten Realisierung wesentlich schneller läuft als wenn man für jedes Beispiel alle Gewichtungen und Biases anpassen würde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zusammenfassung\n",
    "Damit haben wir alles expliziert, was unser neuronales Netz ausmacht! Zusammenfassend ist unser Netz also ein Algorithmus, der einen Inputvektor über mehrere Übergangsmatrizen durch verschiedene Layer, deren Neuronen jeweils gebiast sind, verarbeitet und einen Outputvektor ausgibt, der anzeigt, als welche Ziffer das Netz das Bild des Inputvektors interpretiert. Der Output wird dann mit dem erwarteten Output verglichen, wir ermitteln über die Kostenfunktion \"wie falsch\" der Output des Netzes ist. Die Kostenfunktion wird mithilfe von Stochastischem Gradientenabstieg minimiert. Das heißt, dass die Gewichtungen und Biases in Abhängigkeit des Gradienten der Kostenfunktion einiger Trainingsbeispiele angepasst werden. Unsere Hoffnung ist, dass das Netz Muster in den Trainingsbeispielen erkennt, die sich auf weitere Beispiele verallgemeinern lassen, sodass es auch weitere Beispiele korrekt klassifizieren kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmierung eines einfachen Netzes mit Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden werden wir mithilfe von Python ein einfaches neuronales Netz aufbauen. Wir werden es darauf trainieren, handgeschriebene Zahlen zu erkennen; auch wenn die Anwendung nicht darauf spezialisiert ist.  \n",
    "Python-, Numpy- und Matplotlib-Grundkenntnisse werden hier vorausgesetzt, sowie die Fähigkeit, unbekannte Funktionen mithilfe der entsprechenden Dokumentationen ([Python](https://docs.python.org/3/), [Numpy](https://docs.scipy.org/doc/), [Matplotlib](https://matplotlib.org/contents.html)) zu verstehen.  \n",
    "Der Code des Netzes wird sich stark an [Nielsens (2015)](http://neuralnetworksanddeeplearning.com/chap1.html) Code orientieren. Die meisten Funktionsnamen werden unverändert bleiben, sodass die Orientierung im Code leicht fallen sollte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ein paar notwendige Funktionen vorweg ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Die \"*MNIST database of handwritten digits*\" ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die [MNIST-Datenbank](http://yann.lecun.com/exdb/mnist/) ist eine von Yann LeCun, Corinna Cortes und Christopher J.C. Burges. zusammengestellte Datenbank mit 70,000 Schwarz-Weiß-Bildern von handgeschriebenen Ziffern mit dazugehörigen Labels, aufgeteilt in ein Trainingset mit 60,000 und ein Testset mit 10,000 Bildern und Labels.\n",
    "Die Ziffern in den einzelnen Sets stammen ungefähr zur Hälfte von Highschool-Schülern und zur Hälfte von MitarbeiterInnen des United States Census Bureaus. Geammelt wurden die Daten vom Unites States National Institute of Standards and Technology (NIST) und in zwei Sets, Special Database 1 (mit den von den Highschool-Schülern geschriebenen Ziffern) und Special Database 3 (mit den von den MitarbeiterInnen des Census Bureaus geschriebenen Ziffern) zusammengefasst. \n",
    "Mithilfe des Codes unten deklarieren wir vier Arrays: `x_train` und `x_test` beinhalten die 60,000 bzw. 10,000 Bilder aus dem Trainings- bzw. Testset. Konkret ist jeder Eintrag dieser Arrays ein Array mit 784 Integern von 0 bis 255. Jeder Eintrag repräsentiert den Grauwert eines Pixels des Bildes. `t_train` und `t_test` beinhalten die zu den Bildern gehörigen Ziffern als Integer. `x_train[0]` zum Beispiel ist das Bild einer 5, also ist `t_train[0]` = 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst importieren wir alle notwendigen Python-Bibliotheken und initialisieren die Trainings- und Test-Arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "mnist.init()\n",
    "x_train, t_train, x_test, t_test = mnist.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst einmal wollen wir eine Möglichkeit haben, uns unsere Zahlen anzeigen zu lassen. Wir benutzen dafür imshow aus matplotlib.pyplot.\n",
    "imshow benötigt aber nicht ein Array von der Länge 784 (wie die Arrays in `x_train` und `x_test`), sondern ein 28x28-Array mit den Pixelwerten. Die Konvertierung übernimmt die Funktion *bildconv*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bildconv(bild):\n",
    "    pixellist = []                                      # Wir erstellen eine leere Liste \"pixellist\".\n",
    "    for i in range(28):                                 \n",
    "        pixellist.append([])                            # Wir erstellen 28 Zeilen\n",
    "        for j in range(28):\n",
    "            pixellist[i].append(bild[i*28 + j])         # und füllen diese mit je 28 Einträgen aus dem ursprünglichen Array.\n",
    "    return pixellist                                    # 28x28-array bestehend aus 28 Zeilen mit 28 Werten/Spalten pro Zeile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Den Output dieser Funktion können wir als Eingabewert für imshow aus matplotlib verwenden und \"manuell\" mit dem dazugehörigen Wert aus `t_train` vergleichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In t_train steht, dass das ausgewählte Beispiel aus x_train eine 4 ist.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imnumber = 2 # imnumber wird genutzt, um ein Beispiel aus dem Set x_train auszuwählen, dem an entsprechender Stelle in t_train die geschriebene Ganzzahl korrelliert.\n",
    "    \n",
    "imgplot = plt.imshow(bildconv(x_train[int(imnumber)]), cmap=\"gray\")\n",
    "print(\"In t_train steht, dass das ausgewählte Beispiel aus x_train eine\",t_train[int(imnumber)],\"ist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes brauchen wir eine Funktion, die den erwarteten Output-Vektor des Netzes in Abhängigkeit der erwarteten Zahl ausgibt. In t_train und t_test stehen ja Integer. Unser Netz wird aber einen Vektor der Größe 10 haben, wobei der n-te Eintrag die Zahl n repräsentiert. Um die Einträge in t_train und t_test also mit den Outputs unseres Netzes vergleichen zu können, müssen wir einen Vektor der Größe 10 erstellen, dessen Einträge $ v_t $ für $t\\neq n$  0 sind und für $t = n$ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    temp = np.zeros(10)\n",
    "    temp[j] = 1.0\n",
    "    return temp # 10-Komponenten-Vektor mit dem Wert 1 an der Stelle j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noch zwei notwendige Funktionen: Die Sigmoidfunktion, die wir anstatt einem Threshold benutzen werden, um die Aktivierungen der Neuronen zwischen 0 und 1 zu halten, und ihre Ableitung, die für die Backpropagation benötigt wird. Wir benutzen also keine binären Aktivierungen.\n",
    "\n",
    "$$ Sigmoid(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "\n",
    "Da exp(-x) für kleine x sehr schnell sehr groß wird und Overflow in Python verursacht, benutzen wir eine andere Schreibweise der Sigmoidfunktion. Durch Erweitern des Bruches mit $ e^x $ erhalten wir:\n",
    "\n",
    "$$ Sigmoid(x) = \\frac{e^x}{e^x+1} $$\n",
    "\n",
    "Für kleine x ist die untere Schreibweise numerisch stabiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):                             # Die Sigmoidfunktion, angewandt auf jeden Eintrag eines Vektors\n",
    "    temp = []\n",
    "    for i in z:\n",
    "        temp.append(sigmoidint(i))\n",
    "    return temp\n",
    "def sigmoidint(z):                          # Die Sigmoidfunktion, angewandt auf eine einzelnen Integer\n",
    "    if z >= 0:\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    else:\n",
    "        return np.exp(z) / (1 + np.exp(z))\n",
    "\n",
    "def sigmoid_prime(z):                       # Die erste Ableitung der Sigmoidfunktion, wie oben\n",
    "    temp = []\n",
    "    for i in z:\n",
    "        temp.append(sigmoidint(i)*(1-sigmoidint(i)))\n",
    "    return temp\n",
    "\n",
    "def sigmoid_primeint(z):\n",
    "    return sigmoidint(z)*(1-sigmoidint(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Funktionen des Netzwerkes\n",
    "Wir werden das Netz als Klasse *Network* aufbauen. Im Folgenden werde ich die Methoden der Klasse näher erklären."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *\\_\\_init\\_\\_(self, sizes)* :\n",
    "Das Array sizes besteht aus den Größen der einzelnen Layer unseres Netzwerkes. Dementsprechend werden die Gewichtungen und Biases zufällig gesetzt:\n",
    "```python\n",
    "def __init__(self, sizes):\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(i) for i in sizes[1:]]       \n",
    "        self.weights = [np.random.randn(i, j) for j, i in zip(sizes[:-1], sizes[1:])]                                                                     \n",
    "```  \n",
    "\n",
    "`biases` ist jetzt eine Liste von Arrays, die die (zufällig gesetzten) Biases pro Layer repräsentieren. Die Länge von `biases` entspricht der Länge von `sizes`, die Länge jedes einzelnen Arrays in `biases` entspricht dem entsprechenden Eintrag in `sizes`. So haben wir eine Liste, die alle Biases des Netzes \"layerweise\" speichert. `biases[1]` ist also eine Liste, die die Biases der Neuronen im zweiten Layer unseres Netzes enthält.  \n",
    "Unsere Gewichtungen speichern wir in ähnlicher Weise in `weights`. `weights` ist eine Liste von Matrizen, die ebenfalls layerweise gespeichert werden. Bei `weights[1]` findet sich also die Matrix, die das zweite mit dem dritten layer verbindet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *feedforward(self, a)*\n",
    "Diese Funktion gibt den Output des Netzes in Abhängigkeit eines Inputs a aus. Wir iterieren über alle weights und biases und speichern mit jeder Iteration a die Aktivierungen des jeweils nächsten Layers. Vor der ersten Iteration repräsentiert a einfach nur das Input-Layer, nach der ersten Itreation repräsentiert a das zweite Layer, und so weiter:\n",
    "```python\n",
    "def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.add(np.dot(w, a),b))\n",
    "        return a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *SGD(self, training_data, epochs, mini_batch_size, eta)*\n",
    "`SGD` ist die äußerste Funktion des Lernprozesses des Netzwerkes. `SGD` ist die Funktion, die wir ausführen werden, um das Netzwerk auf den Trainingsdaten `training_data` zu trainieren. Als `training_data` wird hier eine Liste von Tupeln übergeben, die den Input des Netzes und den erwarteten Output des Netzes repräsentieren.  \n",
    "Zuerst mischen wir die Trainingsdaten, dann bilden wir Mini-Batches der Größe `mini_batch_size`:\n",
    "```python\n",
    "random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "```\n",
    "Anschließend updaten wir die Gewichtungen und Biases des Netzes für jedes der gerade konsturierten Mini-Batches mithilfe der Funktion `update_mini_batch` (siehe dort). Diesen Vorgang wiederholen wir sooft, wie in `epochs`angegeben. Außerdem wird nach jeder Epoche eine Bestätigungsmeldung gedruckt, um zu sehen, dass das Programm noch läuft.  \n",
    "Mit `eta` übergibt man die *Lernrate*. Sie bestimmt, wie groß die Anpassungen sind, die wir im Stochastischen Gradientenabstieg machen. In dieser Funktion tun wir noch nichts damit, sondern übergeben `eta` nur an `update_mini_batch`.\n",
    "```python\n",
    "def SGD(self, training_data, epochs, mini_batch_size, eta):\n",
    "        \n",
    "        n = len(training_data)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            print(\"Epoche\",j+1,\"beendet.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *update_mini_batch(self, mini_batch, eta)*\n",
    "`update_mini_batch` passt die Gewichtungen und Biases des Netzwerkes abhängig von einem einzelnen Mini-Batch an:\n",
    "```python\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "```\n",
    "Hier initialisieren wir zwei Listen, die die gleiche Form haben wie `weights` und `biases`.\n",
    "\n",
    "```python\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "                                                                             \n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "```\n",
    "Diese for-Schleife geht das aktuelle Mini-Batch durch. Die Zeile `delta_nabla_b, delta_nabla_w = self.backprop(x, y)` macht die meiste Arbeit der Backpropagation. Die Funktion `backprop` gibt den **positiven** Gradienten der Kostenfunktion für ein Trainingsbeispiel aus. In `nabla_w` bzw. `nabla_b` addieren wir diese Gradienten für das eine Mini-Batch auf.  \n",
    "Für eine Erklärung des Codes von `backprop`, siehe [Nielsen (2015)](http://neuralnetworksanddeeplearning.com/chap2.html), Kap. 2.\n",
    "\n",
    "```python\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "```\n",
    "Als nächstes subtrahieren wir den Durchschnitt der oben aufsummierten Gradienten von den momentanen Gewichtungen und Biases, nachdem wir sie mit `eta` multipliziert haben.\n",
    "\n",
    "\n",
    "```python\n",
    "def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "                                                                                \n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "                                                                                \n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *evaluate(self, test_data)*\n",
    "Diese Funktion geht die Testbeispiele durch und gibt aus, wie viele davon das Netz korrekt klassifiziert. Der höchste Eintrag des Output-Vektors wird hier als die Klassifizierung des Netzes interpretiert.\n",
    "```python\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *cost_derivative(self, output_activations, y)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Funktion wird benutzt, um die partiellen Ableitungen der Kostenfunktion der Neuronen im letzten Layer zu berechnen. Wir benötigen sie nur für die Backpropagation:\n",
    "```python\n",
    "def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist auch alles für den Code der Netzwerk-Klassse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(i) for i in sizes[1:]]       \n",
    "        \n",
    "        self.weights = [np.random.randn(i, j) for j, i in zip(sizes[:-1], sizes[1:])]  \n",
    "                                                \n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.add(np.dot(w, a),b))\n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta):\n",
    "        \n",
    "        n = len(training_data)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            print(\"Epoche\",j+1,\"beendet.\")\n",
    "    \"\"\"Wir wenden Stochastic Gradient Descent auf das Netzwerk an.\"\"\"\n",
    "                \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "                                                                                \n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "                                                                             \n",
    "            \n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "                                                                                \n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Diese Funktion gibt den Gradienten der Kostenfunktion in Bezug auf die weights und biases aus. Die Ausgabe findet als Tupel (nabla_b, nabla_w)\n",
    "        statt, in der gleichen Form wie weights und biases. So können wir den Gradienten in update_mini_batch intuitiv verwenden.\"\"\"\n",
    "        deltas = [np.zeros(i) for i in self.sizes[0:]] # deltas speichert die Fehler, den das Netz pro Layer produziert.\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # In activations speichern wir die Aktivierungen der einzelnen Layer.\n",
    "        zs = [] # In zs speichern wir die einzelnen Layer, bevor die Sigmoid-Funktion angewandt wird.\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        deltas[-1] = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            deltas[-l] = np.dot(np.transpose(self.weights[-l+1]), deltas[-l+1]) * sp\n",
    "\n",
    "        for l in range(len(nabla_w)):\n",
    "            nabla_w[l] = np.outer(deltas[l+1], np.asarray(activations[l]).transpose())\n",
    "            \n",
    "        return (deltas[1:], nabla_w)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als nächstes müssen wir `training_data` und `test_data` initialisieren. Die Input-Vektoren müssen noch durch 255 geteilt werden, da `x_train` und `x_test` Einträge zwischen 0 und 255 haben, wir aber Inputs zwischen 0 und 1 erwarten. `vectorized_result` wurde oben schon erklärt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = []\n",
    "test_data = []\n",
    "\n",
    "for i, j in zip(x_train, t_train):\n",
    "    training_data.append((i/255,vectorized_result(j)))\n",
    "    \n",
    "for i, j in zip(x_test, t_test):\n",
    "    test_data.append((i/255,vectorized_result(j)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann sind wir jetzt fertig, unser erstes Netzwerk zu erstellen. Versuchen wir es mit einem Hidden Layer mit 20 Neuronen. Außerdem können wir uns den noch völlig zufälligen Output des Netzes zusammen mit dem dazugehörigen Bild aus x_train ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mein Output-Vektor lautet: [0.3436923823959011, 0.12085544668826313, 0.9280060673539101, 0.6784690054513262, 0.9700366810258868, 0.11128251044477146, 0.7903364077973608, 0.7973866860878795, 0.24135219158614366, 0.9873984550024564]\n",
      "Ich bin mir zu 98.73984550024564 % sicher, dass das eine 9 ist. Eigentlich ist es eine 0 .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = Network([784, 20, 10])\n",
    "example = 1\n",
    "outp = net.feedforward(x_train[example])\n",
    "print(\"Mein Output-Vektor lautet:\", outp)\n",
    "print(\"Ich bin mir zu\", np.amax(outp)*100,\"% sicher, dass das eine\",np.argmax(outp),\"ist. Eigentlich ist es eine\", t_train[example],\".\")\n",
    "imgplot = plt.imshow(bildconv(x_train[example]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Versuchen wir nun, das Netzwerk mithilfe der MNIST-Training-Data zu trainieren. Versuchen wir es mit 10 Epochen, einer Mini-Batch-Größe von 10 und einer Lernrate eta = 3.0. Anschließend lassen wir uns ausgeben, wie gut das Netzwerk gelernt hat. Das Training kann hier einige Minuten dauern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoche 1 beendet.\n",
      "Epoche 2 beendet.\n",
      "Epoche 3 beendet.\n",
      "Epoche 4 beendet.\n",
      "Epoche 5 beendet.\n",
      "Epoche 6 beendet.\n",
      "Epoche 7 beendet.\n",
      "Epoche 8 beendet.\n",
      "Epoche 9 beendet.\n",
      "Epoche 10 beendet.\n",
      "Ich habe von 10000 Beispielen im Test-Set 9355 richtig klassifiziert. Das entspricht einer Genauigkeit von 93.55 %.\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, epochs = 10, mini_batch_size = 10, eta = 3.0)\n",
    "\n",
    "evaluation = net.evaluate(test_data)\n",
    "\n",
    "print(\"Ich habe von\",len(test_data),\"Beispielen im Test-Set\",evaluation,\"richtig klassifiziert. Das entspricht einer Genauigkeit von\",(evaluation/len(test_data))*100,\"%.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir uns auch den Output des Netzes für ein einzelnes Beispiel ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.565601548520264e-09, 0.00023307031395709542, 8.288282250549109e-06, 0.3572625156925892, 2.393826283859824e-05, 1.7640706941194546e-07, 7.31821767183534e-08, 4.2240130831259875e-06, 0.3061502595120801, 0.8668669133024435]\n",
      "Ich bin mir zu 86.68669133024434 % sicher, dass das eine 9 ist.\n",
      "Eigentlich ist es eine 9 .\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example = 7\n",
    "outp = net.feedforward(x_test[example])\n",
    "print(outp)\n",
    "print(\"Ich bin mir zu\", np.amax(outp)*100,\"% sicher, dass das eine\",np.argmax(outp),\"ist.\")\n",
    "imgplot = plt.imshow(bildconv(x_test[example]), cmap=\"gray\")\n",
    "print(\"Eigentlich ist es eine\",t_test[example],\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der folgenden Codezelle kann mit den Netzwerkkonfigurationen etwas herumexperimentiert werden und beobachtet werden, wie sich die Anzahl und Größe der Hidden Layer auf die Performance des Netzes und die Dauer des Lernprozesses auswirkt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net2 = Network([784, 300, 200, 10])\n",
    "\n",
    "net2.SGD(training_data, epochs = 2, mini_batch_size = 10, eta = 3.0)\n",
    "\n",
    "evaluation = net2.evaluate(test_data)\n",
    "\n",
    "print(\"Ich habe von\",len(test_data),\"Beispielen im Test-Set\",evaluation,\"richtig klassifiziert. Das entspricht einer Genauigkeit von\",(evaluation/len(test_data))*100,\"%.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = 9888\n",
    "outp = net2.feedforward(x_test[example])\n",
    "print(outp)\n",
    "print(\"Ich bin mir zu\", np.amax(outp)*100,\"% sicher, dass das eine\",np.argmax(outp),\"ist.\")\n",
    "imgplot = plt.imshow(bildconv(x_test[example]), cmap=\"gray\")\n",
    "print(\"Eigentlich ist es eine\",t_test[example],\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit diesem relativ einfachen Netz können wir schon überraschend gute Ergebnisse erzielen. Es kann zwar noch verfeinert werden (siehe dazu [Nielsen, Kap. 3](http://neuralnetworksanddeeplearning.com/chap3.html)), aber die simple Aufgabe, die wir ihm gestellt haben, kann es einigermaßen gut lösen.  \n",
    "Die relativ basale Idee, die wir hier angewandt haben, kann aber auch noch verfeinert werden. Vor allem für Sprach- und Musikerkennung bieten sich sogenannte LSTM-Netze (*long short-term memory*-Netze) an. Die Idee hierbei ist, einen Teil des Outputs zu verwenden, um den nächsten Input zu verarbeiten. Zum Beispiel, wenn es darum geht, ein Konzept - wie ein Gesprächsthema - über eine gewisse Zeitdistanz im Gedächtnis zu behalten, kann diese Idee Früchte tragen. Außerdem gibt es noch einige andere Arten neuronaler Netze, die für verschiedene Aufgaben mehr oder weniger geeignet sind.  \n",
    "Die meisten dieser Netze beruhen - wie unser Netz auch - auf der Idee des *supervised learning*. Das heißt, dass der Output des Netzes verglichen wird mit einer Erwartung, die wir an das Netz haben. Alternativ dazu gibt es *unsupervised learning*, bei dem ein Netz Muster in den Trainingsdaten erkennt, ohne dass eine Erwartung an das Netz gestellt wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literatur\n",
    "\n",
    "* LeCun, Y., Cortes, C. and Burges, C. (2018). MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges. URL: http://yann.lecun.com/exdb/mnist/ \\[Zuletzt aufgerufen am 05.10.2018\\].  \n",
    "* Nielsen, M. (2018). Neural Networks and Deep Learning. URL: http://neuralnetworksanddeeplearning.com/ \\[Zuletzt aufgerufen am 5 Oct. 2018\\].\n",
    "* Docs.python.org. (2018). 3.7.1rc1 Documentation. URL: https://docs.python.org/3/ \\[Zuletzt aufgerufen am 5 Oct. 2018\\].\n",
    "* Docs.scipy.org. (2018). Numpy and Scipy Documentation — Numpy and Scipy documentation. URL: https://docs.scipy.org/doc/ \\[Zuletzt aufgerufen am 5 Oct. 2018\\].\n",
    "* Matplotlib.org. (2018). Overview — Matplotlib 3.0.0 documentation. URL: https://matplotlib.org/contents.html \\[Zuletzt aufgerufen am 5 Oct. 2018\\]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3pack",
   "language": "python",
   "name": "py3pack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
